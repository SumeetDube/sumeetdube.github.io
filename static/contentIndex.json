{"Deep-Learning/Broadcasting-Semantics-in-Pytorch":{"title":"Broadcasting Semantics in Pytorch","links":[],"tags":["DeepLearning","Pytorch"],"content":"PyTorch is the most popular open-source deep learning framework developed by Facebook’s AI Research lab. It provides a flexible and intuitive platform for building and training neural networks, with strong support for dynamic computational graphs and automatic differentiation. PyTorch is widely used in both research and production due to its ease of use, extensive libraries, and active community.\nBroadcasting Semantics\nBroadcasting semantics in the context of arrays and tensors refers to a set of rules that allow operations on arrays of different shapes and sizes without explicitly replicating data. When performing element-wise operations, smaller arrays are “broadcast” across the larger array so that their shapes are compatible. This enables efficient and concise computations, avoiding unnecessary memory usage.\nBroadcasting semantics in PyTorch\nPyTorch derives it’s broadcasting semantics from NumPy. It allows its Tensor arguments to be automatically expanded to be of equal size for computing efficient matrix multiplications.\nGeneral Rules\n\nEach tensor has at least one dimension.\nThe dimension sizes must either be equal, one of them is 1, or one of them does not exist. Always start from the trailing end.\n\nExamples :\n\n\nSame shapes are always broadcastable (i.e. the above rules always hold)\nx = torch.empty(5,7,3)\ny = torch.empty(5,7,3)\n(x+y).size()\n\ntorch.Size([5, 7, 3])\n\n\n\nOne of the dimensions is 1\nx = torch.empty((2,2))\ny = torch.empty (1,2)\n(x+y).size()\n\ntorch.Size([2, 2])\n\n\n\nOne dimension doesn’t exist.\nx=torch.empty(5,3,4,1)\ny=torch.empty(  3,1,1)\n(x + y).size()\n\ntorch.Size([5, 3, 4, 1])\n\n\n1st and 2nd trailing dimension: both have size 1\n3rd trailing dimension: x size == y size\n4th trailing dimension: y dimension doesn&#039;t exist. So x_size = y_size which is 5.\n\n\n\nIf two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n\n\nIf the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n\n\nThen, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n\n\nHere are some more examples:\nx=torch.empty(5,1,4,1)\ny=torch.empty(  3,1,1)\n(x+y).size()\n\ntorch.Size([5, 3, 4, 1])\n\nx=torch.empty(1)\ny=torch.empty(3,1,7)\n(x+y).size()\n\n\ntorch.Size([3, 1, 7])\n\nx=torch.empty(1,3,1)\ny=torch.empty(3,1,7)\n(x + y).size()\n\ntorch.Size([3, 3, 7])\n\nx=torch.empty(1,3,1)\ny=torch.empty(3,1,7)\n(x + y).size()\n\ntorch.Size([3, 3, 7])\n\nExamples of wrong operations\n\n\nx and y are not broadcastable, because x does not have at least 1 dimension\nx=torch.empty((0,))\ny=torch.empty(2,2)\n\n\nx and y are not broadcastable, because in the 2nd trailing dimension 2 != 3\nx=torch.empty(5,2,4)\ny=torch.empty(  3,1)\n\n\n\nSources\npytorch.org/docs/stable/notes/broadcasting.html\nnumpy.org/doc/stable/user/basics.broadcasting.html"},"Deep-Learning/Introduction-to-ResNet":{"title":"Introduction to ResNet","links":[],"tags":["DeepLearning"],"content":"ResNet, short for Residual Network, is a type of artificial neural network architecture introduced by Kaiming He and his colleagues in their 2015 paper “Deep Residual Learning for Image Recognition.” ResNet has been highly influential in the field of deep learning, particularly for its ability to train very deep networks effectively. Here’s a detailed overview:\nKey Concepts\n\n\nResidual Learning:\n\nThe core idea of ResNet is to use residual learning to make training deep networks easier. Instead of trying to learn the underlying mapping directly, ResNet learns the residuals, i.e., the difference between the desired mapping and the input.\nMathematically, if the desired mapping is ( H(x) ), ResNet lets the layers learn a residual function ( F(x) = H(x) - x ), which can be rephrased to ( H(x) = F(x) + x ).\n\n\n\nSkip Connections (or Shortcut Connections):\n\nResNet introduces skip connections that bypass one or more layers. These connections add the input of a layer to the output of a deeper layer.\nThis helps mitigate the vanishing gradient problem by allowing gradients to flow directly through the network without passing through non-linear activations, making the optimization process more efficient.\n\n\n\nArchitecture\n\nA typical ResNet architecture consists of multiple residual blocks. A residual block can be represented as follows:\n\n\n\nConvolutional Layers:\n\nEach residual block usually consists of a few convolutional layers. For example, in ResNet-50, there are 3 layers per block, while in ResNet-34, there are 2 layers per block.\n\n\n\nBatch Normalization:\n\nBatch normalization is applied after each convolutional layer to normalize the activations, which helps in faster convergence and more stable training.\n\n\n\nActivation Function:\n\nTypically, ReLU (Rectified Linear Unit) is used as the activation function.\n\n\n\nAddition Operation:\n\nThe input (via the skip connection) is added to the output of the convolutional layers before passing through the next block.\n\n\n\nVariants\nResNet has several variants, mainly differing in depth:\n\nResNet-18: 18 layers\nResNet-34: 34 layers\nResNet-50: 50 layers\nResNet-101: 101 layers\nResNet-152: 152 layers\n\nThe deeper variants (e.g., ResNet-50, ResNet-101) use bottleneck architectures, where each block has three layers: a 1x1 convolution to reduce dimensions, a 3x3 convolution, and a 1x1 convolution to restore dimensions.\nAdvantages\n\n\nDeeper Networks:\n\nResNet allows the training of very deep networks (hundreds or even thousands of layers) by mitigating issues like vanishing gradients.\n\n\n\nImproved Performance:\n\nIt has significantly improved the performance of deep learning models on various tasks, particularly in image recognition, achieving state-of-the-art results on benchmarks like ImageNet.\n\n\n\nGeneralization:\n\nResidual connections improve the generalization capabilities of the model, making it more robust.\n\n\n\nApplications\nResNet architectures are widely used in various computer vision tasks, including:\n\nImage classification\nObject detection\nSemantic segmentation\nImage generation\n\nIn summary, ResNet’s introduction of residual learning and skip connections has made it a foundational architecture in deep learning, enabling the successful training of very deep networks and achieving superior performance across many tasks."},"Deep-Learning/Neural-Networks":{"title":"Neural Networks","links":[],"tags":["DeepLearning"],"content":"\nA neural network is a computational model inspired by the way biological neural networks in the human brain process information. These networks are a key component of artificial intelligence and machine learning, designed to recognize patterns, learn from data, and make decisions.\nBasic Concepts and Structure\n\n\nNeurons (Nodes):\n\nThe fundamental units of a neural network are called neurons or nodes.\nEach neuron receives inputs, processes them, and passes on an output.\n\n\n\nLayers:\n\nInput Layer: The first layer that receives the input data.\nHidden Layers: Layers between the input and output layers where computations occur. There can be multiple hidden layers.\nOutput Layer: The final layer that produces the output.\n\n\n\nWeights and Biases:\n\nWeights are parameters that adjust the input’s importance.\nEach connection between neurons has an associated weight.\nBiases are additional parameters added to the neuron’s input to help adjust the output.\n\n\n\nActivation Function:\n\nEach neuron applies an activation function to its input to produce an output.\nCommon activation functions include the sigmoid, tanh, ReLU (Rectified Linear Unit), and softmax.\n\n\n\n\nWorking Mechanism\n\n\nForward Propagation:\n\nInput data is fed into the input layer.\nData passes through the hidden layers, where neurons apply weights, biases, and activation functions to compute their outputs.\nThe final output is produced by the output layer.\n\n\n\nLoss Function:\n\nThe network’s output is compared with the actual target values using a loss function (e.g., mean squared error, cross-entropy).\nThe loss function quantifies the difference between the predicted output and the actual output.\n\n\n\nBackpropagation:\n\nThe network adjusts the weights and biases to minimize the loss.\nThis is done by propagating the error backward through the network, computing the gradient of the loss function with respect to each weight and bias (using techniques like gradient descent).\n\n\n\nTypes of Neural Networks\n\n\nFeedforward Neural Network (FNN):\n\nThe simplest type, where connections between the nodes do not form cycles.\nInformation moves in one direction, from input to output.\n\n\n\nConvolutional Neural Network (CNN):\n\nSpecialized for processing structured grid data like images.\nUtilizes convolutional layers to detect patterns such as edges and textures.\n\n\n\nRecurrent Neural Network (RNN):\n\nDesigned for sequential data like time series or natural language.\nFeatures connections that form directed cycles, allowing information to persist.\n\n\n\nLong Short-Term Memory (LSTM):\n\nA type of RNN that can learn long-term dependencies.\nAddresses the vanishing gradient problem in traditional RNNs.\n\n\n\nGenerative Adversarial Network (GAN):\n\nConsists of two networks: a generator and a discriminator.\nThe generator creates data, and the discriminator evaluates it, allowing the network to produce realistic data.\n\n\n\nApplications\n\nImage Recognition: Used in facial recognition, medical image analysis, and object detection.\nNatural Language Processing (NLP): Enables machine translation, sentiment analysis, and text generation.\nSpeech Recognition: Converts spoken language into text.\nAutonomous Systems: Powers self-driving cars and drones.\nFinancial Forecasting: Used for stock market prediction and risk management.\nHealthcare: Assists in diagnosis, personalized medicine, and drug discovery.\n"},"index":{"title":"Sumeet Dube","links":["Deep-Learning/Broadcasting-Semantics-in-Pytorch","Deep-Learning/Introduction-to-ResNet","Deep-Learning/Neural-Networks","Deep-Learning/"],"tags":[],"content":"\n \n\nHi there! I’m Sumeet, an engineering student with a passion for Robotics.\nThis website is a showcase of my projects, skills, and experiences in Machine Learning, Robotics, and Computer Vision. I’m thrilled to share my work with you, and I hope you find it both informative and inspiring.\nFeel free to explore some of the articles I’ve written. Enjoy your visit!\nFollow me on\nLinkedin | Github | X\n\nPosts\n\nBroadcasting Semantics in Pytorch\nIntroduction to ResNet\nWhat are Neural Networks\n\nDeep Learning"}}