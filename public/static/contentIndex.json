{"Posts/Introduction-to-ResNet":{"title":"Introduction to ResNet","links":[],"tags":["DeepLearning"],"content":"ResNet, short for Residual Network, is a type of artificial neural network architecture introduced by Kaiming He and his colleagues in their 2015 paper “Deep Residual Learning for Image Recognition.” ResNet has been highly influential in the field of deep learning, particularly for its ability to train very deep networks effectively. Here’s a detailed overview:\nKey Concepts\n\n\nResidual Learning:\n\nThe core idea of ResNet is to use residual learning to make training deep networks easier. Instead of trying to learn the underlying mapping directly, ResNet learns the residuals, i.e., the difference between the desired mapping and the input.\nMathematically, if the desired mapping is ( H(x) ), ResNet lets the layers learn a residual function ( F(x) = H(x) - x ), which can be rephrased to ( H(x) = F(x) + x ).\n\n\n\nSkip Connections (or Shortcut Connections):\n\nResNet introduces skip connections that bypass one or more layers. These connections add the input of a layer to the output of a deeper layer.\nThis helps mitigate the vanishing gradient problem by allowing gradients to flow directly through the network without passing through non-linear activations, making the optimization process more efficient.\n\n\n\nArchitecture\n\nA typical ResNet architecture consists of multiple residual blocks. A residual block can be represented as follows:\n\n\n\nConvolutional Layers:\n\nEach residual block usually consists of a few convolutional layers. For example, in ResNet-50, there are 3 layers per block, while in ResNet-34, there are 2 layers per block.\n\n\n\nBatch Normalization:\n\nBatch normalization is applied after each convolutional layer to normalize the activations, which helps in faster convergence and more stable training.\n\n\n\nActivation Function:\n\nTypically, ReLU (Rectified Linear Unit) is used as the activation function.\n\n\n\nAddition Operation:\n\nThe input (via the skip connection) is added to the output of the convolutional layers before passing through the next block.\n\n\n\nVariants\nResNet has several variants, mainly differing in depth:\n\nResNet-18: 18 layers\nResNet-34: 34 layers\nResNet-50: 50 layers\nResNet-101: 101 layers\nResNet-152: 152 layers\n\nThe deeper variants (e.g., ResNet-50, ResNet-101) use bottleneck architectures, where each block has three layers: a 1x1 convolution to reduce dimensions, a 3x3 convolution, and a 1x1 convolution to restore dimensions.\nAdvantages\n\n\nDeeper Networks:\n\nResNet allows the training of very deep networks (hundreds or even thousands of layers) by mitigating issues like vanishing gradients.\n\n\n\nImproved Performance:\n\nIt has significantly improved the performance of deep learning models on various tasks, particularly in image recognition, achieving state-of-the-art results on benchmarks like ImageNet.\n\n\n\nGeneralization:\n\nResidual connections improve the generalization capabilities of the model, making it more robust.\n\n\n\nApplications\nResNet architectures are widely used in various computer vision tasks, including:\n\nImage classification\nObject detection\nSemantic segmentation\nImage generation\n\nIn summary, ResNet’s introduction of residual learning and skip connections has made it a foundational architecture in deep learning, enabling the successful training of very deep networks and achieving superior performance across many tasks."},"index":{"title":"Hello","links":["Posts/Introduction-to-ResNet"],"tags":[],"content":"\n \n\nWelcome to my website. I’m you friendly neighborhood programmer with a passion for open-source. This website showcases my projects, skills, and experiences in the field of machine learning. I’m excited to share my work with you and hope you find it informative and inspiring.\nSocials\nLinkedin | Github | X\n\nPosts\nIntroduction to ResNet"}}